{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTjKRKjcsQFW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q langchain \\\n",
        "langchain_openai \\\n",
        "arxiv \\\n",
        "PyMuPDF \\\n",
        "chromadb \\\n",
        "langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsPqQVs0r0w9",
        "outputId": "5b612fd7-bb05-48e2-ebb8-45611550e55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Flavor-violating axion couplings are constrained by high-intensity laboratory experiments\\nlooking for missing energy in rare decays. The strongest constraint is set by the experiments' metadata={'Published': '2024-03-08', 'Title': 'Thermal production of astrophobic axions', 'Authors': 'Marcin Badziak, Keisuke Harigaya, Michał Łukawski, Robert Ziegler', 'Summary': 'Hot axions are produced in the early Universe via their interactions with\\nStandard Model particles, contributing to dark radiation commonly parameterized\\nas $\\\\Delta N_{\\\\text{eff}}$. In standard QCD axion benchmark models, this\\ncontribution to $\\\\Delta N_{\\\\text{eff}}$ is negligible after taking into account\\nastrophysical limits such as the SN1987A bound. We therefore compute the axion\\ncontribution to $\\\\Delta N_{\\\\text{eff}}$ in so-called astrophobic axion models\\ncharacterized by strongly suppressed axion couplings to nucleons and electrons,\\nin which astrophysical constraints are relaxed and $\\\\Delta N_{\\\\text{eff}}$ may\\nbe sizable. We also construct new astrophobic models in which axion couplings\\nto photons and/or muons are suppressed as well, allowing for axion masses as\\nlarge as few eV. Most astrophobic models are within the reach of CMB-S4, while\\nsome allow for $\\\\Delta N_{\\\\text{eff}}$ as large as the current upper bound from\\nPlanck and thus will be probed by the Simons Observatory. The majority of\\nastrophobic axion models predicting large $\\\\Delta N_{\\\\text{eff}}$ is also\\nwithin the reach of IAXO or even BabyIAXO.'}\n",
            "count before 758\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################################################################################\n",
        "#check is mine works: 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\n",
        "#Pawel: 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\n",
        "\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]='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'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "docs = ArxivLoader(query=\"2403.05621\", load_max_docs=2).load() #TODO load more\n",
        "#2202.10488\n",
        "#2012.06566\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20) #TODO: play around with those parameters, perhaps larger is\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "vectorstore.delete_collection()\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "print(\"count before\", vectorstore._collection.count())\n",
        "\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "llm=ChatOpenAI(temperature=0.1,model_name=\"gpt-3.5-turbo-16k\")  #gpt-3.5-turbo-16k; gpt-4\n",
        "\n",
        "\n",
        "  #can also use Gemini:  https://python.langchain.com/docs/integrations/chat/google_generative_ai\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"QuizQuestion\", description=\"generated question 1\"),\n",
        "    ResponseSchema(\n",
        "        name=\"correctAnswer\",\n",
        "        description=\"correct answer to 1 generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"IncorrectAnswer1\",\n",
        "        description=\"incorrect answer to 1 generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"IncorrectAnswer2\",\n",
        "        description=\"Second incorrect answer to 1 generated question\",\n",
        "    ),\n",
        "    ResponseSchema(name=\"QuizQuestion2\", description=\"generated question\"),\n",
        "    ResponseSchema(\n",
        "        name=\"correctAnswer_2\",\n",
        "        description=\"correct answer to 2 generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"IncorrectAnswer1_2\",\n",
        "        description=\"incorrect answer to 2 generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"IncorrectAnswer2_2\",\n",
        "        description=\"Second incorrect answer to 2 generated question\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "#quiz with closed questions, one for every chapter of the paper in context.\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "format_instructions = output_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL_CVhePrQ8P",
        "outputId": "74817d40-3193-4277-8dd7-39935326e45e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'QuizQuestion': 'How many distinct models are found in the paper?', 'correctAnswer': '7', 'IncorrectAnswer1': '5', 'IncorrectAnswer2': '9', 'QuizQuestion2': 'What is the value of Cc for the Q3 model?', 'correctAnswer_2': '8/3', 'IncorrectAnswer1_2': '14/3', 'IncorrectAnswer2_2': '2/3'}\n",
            "--- 2.136594772338867 seconds ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "template = \"\"\" You are an academic teacher preparing graduation test based on the paper in the context.\n",
        "Do not ask questions about the structure of the paper such as titles of sections or number of section. Ask about details of the paper.\n",
        "The quiz should be difficult, concerning details of the paper.\n",
        "\n",
        "This is the paper about which you will ask two questions: {context}\n",
        "\n",
        "Aks for quiz: {quiz}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Quiz:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template, partial_variables={\"format_instructions\": format_instructions},)\n",
        "\n",
        "# use JSON for output https://python.langchain.com/docs/modules/model_io/output_parsers/types/json\n",
        "# https://python.langchain.com/docs/modules/model_io/output_parsers/types/structured?fbclid=IwAR3paKudjIJF2m2CMe3ZD5t7Eg2F0H9qhBZAfTFPSKh38sEjn1D27y8RT4I\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"quiz\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | output_parser #StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for s in rag_chain.stream(\"Quiz!\"):\n",
        "    print(s)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "ZM_um3wInStk",
        "outputId": "263db1c4-be62-41a0-ad21-2aff93f055ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count before 2639\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\n                You must provide either ids, where, or where_document to delete. If\n                you want to delete all data in a collection you can delete the\n                collection itself using the delete_collection method. Or alternatively,\n                you can get() all the relevant ids and then delete them.\n                ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-12b2a618c113>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count before\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, ids, where, where_document)\u001b[0m\n\u001b[1;32m    519\u001b[0m         )\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     def _validate_embedding_set(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_delete\u001b[0;34m(self, collection_id, ids, where, where_document)\u001b[0m\n\u001b[1;32m    601\u001b[0m             )\n\u001b[1;32m    602\u001b[0m         ):\n\u001b[0;32m--> 603\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \"\"\"\n\u001b[1;32m    605\u001b[0m                 \u001b[0mYou\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mprovide\u001b[0m \u001b[0meither\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwhere_document\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n                You must provide either ids, where, or where_document to delete. If\n                you want to delete all data in a collection you can delete the\n                collection itself using the delete_collection method. Or alternatively,\n                you can get() all the relevant ids and then delete them.\n                "
          ]
        }
      ],
      "source": [
        "ids = [str(i) for i in range(1, len(splits) + 1)]\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "print(\"count before\", vectorstore._collection.count())\n",
        "\n",
        "vectorstore._collection.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzQorSUW__VD"
      },
      "outputs": [],
      "source": [
        "\n",
        "template = \"\"\" You're a diligent researcher who has a keen interest in creating interactive quizzes to help others grasp complex scientific concepts more effectively. Your attention to detail and ability to simplify intricate information make you an ideal quiz maker.\n",
        "Your task is to prepare a quiz with 2 questions based on the paper in the context. Incorporate key points, methodologies, and findings from the paper into the quiz to ensure a comprehensive assessment.\n",
        "Remember to keep the questions clear, concise, and engaging to maintain the reader's interest throughout the quiz.\n",
        "\n",
        "{context}\n",
        "\n",
        "Ask for quiz: {quiz}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Quiz:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template, partial_variables={\"format_instructions\": format_instructions},)\n",
        "\n",
        "# use JSON for output https://python.langchain.com/docs/modules/model_io/output_parsers/types/json\n",
        "# https://python.langchain.com/docs/modules/model_io/output_parsers/types/structured?fbclid=IwAR3paKudjIJF2m2CMe3ZD5t7Eg2F0H9qhBZAfTFPSKh38sEjn1D27y8RT4I\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"quiz\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | output_parser #StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for s in rag_chain.stream(\"Quiz!\"):\n",
        "    print(s)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0LtlP_uBUou"
      },
      "source": [
        "XD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgHZjX7iKfaU",
        "outputId": "a44aff13-1027-4625-a553-fcbe3becbf11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'QuizQuestion': 'What is the total number of distinct models found in the paper?', 'correctAnswer': '7', 'IncorrectAnswer1': '5', 'IncorrectAnswer2': '9', 'QuizQuestion2': 'What are the values of Cc, Cs, Ct, Cb for the Q3 model?', 'correctAnswer_2': '8/3, -1/3, -2/3, 2/3', 'IncorrectAnswer1_2': '14/3, 2/3, -2/3, 2/3', 'IncorrectAnswer2_2': '2/3, -1/3, 1/3, -1/3'}\n",
            "--- 2.2225325107574463 seconds ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "template = \"\"\"You will prepare a quiz based on the paper in the context: {context}\n",
        "You want to ask 2 questions, provided with both correct and incorrect ansewrs, about the content of this paper in form of a quiz in following format: {format_instructions}\n",
        "You should ask about the results in the paper such as general conclusions but not about the structure of the paper.\n",
        "When you will see word \"Quiz!\" you should output the quiz: {quiz}\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template, partial_variables={\"format_instructions\": format_instructions},)\n",
        "\n",
        "\n",
        "# use JSON for output https://python.langchain.com/docs/modules/model_io/output_parsers/types/json\n",
        "# https://python.langchain.com/docs/modules/model_io/output_parsers/types/structured?fbclid=IwAR3paKudjIJF2m2CMe3ZD5t7Eg2F0H9qhBZAfTFPSKh38sEjn1D27y8RT4I\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"quiz\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | output_parser #StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for s in rag_chain.stream(\"Quiz!\"):\n",
        "    print(s)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xBEsmedehG3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
