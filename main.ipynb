{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papeye/qArXiv/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT8bZKNthd17"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q langchain \\\n",
        "langchain_openai \\\n",
        "arxiv \\\n",
        "PyMuPDF \\\n",
        "chromadb \\\n",
        "langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z39VnuygaJx"
      },
      "source": [
        "###Enter doi of your paper below or chose one of the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsPqQVs0r0w9",
        "outputId": "f1af8d79-b324-4de3-fe21-23a3410de286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "What is the SUSY breaking Majorana mass of ˜X referred to in the paper? \n",
            " \n",
            "        m ˜X (correct) \n",
            " \n",
            "        vS ˜S (incorrect) \n",
            "\n",
            "        2m ˜X ˜X ˜X (incorrect) \n",
            "\n",
            "        \n",
            "\n",
            "What is the SUSY breaking Majorana mass of ˜X referred to in the paper? \n",
            " \n",
            "        Z (correct)\n",
            "\n",
            "        X (incorrect) \n",
            "\n",
            "        S (incorrect) \n",
            "\n",
            "        \n",
            "--- 5.322651147842407 seconds ---\n"
          ]
        }
      ],
      "source": [
        "doi = '2202.10488 - Charged Dark Matter in Supersymmetric Twin Higgs models' # @param [\"2202.10488 - Charged Dark Matter in Supersymmetric Twin Higgs models\"] {allow-input: true}\n",
        "model = 'gpt-4' # @param [\"gpt-3.5-turbo-16k\", \"gpt-4\"]\n",
        "\n",
        "doi = doi.split()[0]\n",
        "\n",
        "import time\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '<Your OPENAI_API_KEY>'\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]='<Your HUGGINGFACEHUB_API_TOKES>'\n",
        "\n",
        "docs = ArxivLoader(query=doi, load_max_docs=2).load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "llm=ChatOpenAI(temperature=0.1,model_name=model)\n",
        "#can also use Gemini:  https://python.langchain.com/docs/integrations/chat/google_generative_ai\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"question1\", description=\"question 1\"),\n",
        "    ResponseSchema(\n",
        "        name=\"correctAnswer1\",\n",
        "        description=\"correct answer to the 1st generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"incorrectAnswer11\",\n",
        "        description=\"incorrect answer to the 1st generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"incorrectAnswer12\",\n",
        "        description=\"second incorrect answer to the 1st generated question\",\n",
        "    ),\n",
        "    ResponseSchema(name=\"question2\", description=\"question 2\"),\n",
        "    ResponseSchema(\n",
        "        name=\"correctAnswer2\",\n",
        "        description=\"correct answer to the 2nd generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"incorrectAnswer21\",\n",
        "        description=\"incorrect answer to the 2nd generated question\",\n",
        "    ),\n",
        "    ResponseSchema(\n",
        "        name=\"incorrectAnswer22\",\n",
        "        description=\"second incorrect answer to the 2nd generated question\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Prompt dependent on user\n",
        "template = \"\"\"You will prepare a quiz based on the following context which is a scientific paper: {context}\n",
        "You want to ask 2 questions, provided with both correct and incorrect ansewrs, about the content of this paper in form of a quiz in following format: {format_instructions}\n",
        "You should ask about the results in the paper such as general conclusions but not about the structure of the paper.\n",
        "When you will see word \"Quiz!\" you should output the quiz: {quiz}\"\"\"\n",
        "\n",
        "custom_rag_prompt = PromptTemplate.from_template(template, partial_variables={\"format_instructions\": format_instructions},)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"quiz\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "\n",
        "class Output:\n",
        "  def __init__(self, output):\n",
        "        self.question1 = output['question1']\n",
        "        self.question2 = output['question2']\n",
        "        self.correctAnswer1 = output['correctAnswer1']\n",
        "        self.correctAnswer2 = output['correctAnswer2']\n",
        "        self.incorrectAnswer11 = output['incorrectAnswer11']\n",
        "        self.incorrectAnswer12 = output['incorrectAnswer12']\n",
        "        self.incorrectAnswer21 = output['incorrectAnswer21']\n",
        "        self.incorrectAnswer22 = output['incorrectAnswer22']\n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "        return f'''\n",
        "{self.question1} \\n\n",
        "        {self.correctAnswer1} (correct) \\n\n",
        "        {self.incorrectAnswer11} (incorrect) \\n\n",
        "        {self.incorrectAnswer12} (incorrect) \\n\n",
        "        \\n\n",
        "{self.question1} \\n\n",
        "        {self.correctAnswer2} (correct)\\n\n",
        "        {self.incorrectAnswer21} (incorrect) \\n\n",
        "        {self.incorrectAnswer22} (incorrect) \\n\n",
        "        '''\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "output = Output(rag_chain.invoke(\"Quiz!\"))\n",
        "\n",
        "print(output)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "vectorstore.delete_collection()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}